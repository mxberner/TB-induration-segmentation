{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73fefc27",
   "metadata": {},
   "source": [
    "# U-Net Segmentation with COCO JSON Masks (Local, PyTorch)\n",
    "\n",
    "This notebook trains a **U-Net** segmentation model where **masks are stored in a COCO JSON file** (e.g. Roboflow export),\n",
    "not as separate PNGs.\n",
    "\n",
    "Pipeline:\n",
    "\n",
    "1. Setup & imports  \n",
    "2. Load COCO JSON & build index  \n",
    "3. Dataset class (image + mask from COCO) + Albumentations  \n",
    "4. U-Net model  \n",
    "5. Training & validation loop (Dice + BCEWithLogits)  \n",
    "6. Optional: visualize predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f074ef56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, install these once in your environment (NOT Colab-specific):\n",
    "# !pip install torch torchvision albumentations opencv-python matplotlib pycocotools\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from pycocotools import mask as mask_utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bce836",
   "metadata": {},
   "source": [
    "## CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eb74be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Root directory containing images + COCO JSON\n",
    "# Example structure:\n",
    "#   tb_data/\n",
    "#     images/\n",
    "#       scene01_img01.jpg\n",
    "#       ...\n",
    "#     _annotations.coco.json\n",
    "\n",
    "DATA_ROOT  = Path(\"data\")   # <-- change to your folder\n",
    "IMAGES_DIR = DATA_ROOT / \"\"\n",
    "COCO_JSON  = DATA_ROOT / \"_annotations.coco.json\"\n",
    "\n",
    "# Training hyperparameters\n",
    "IMAGE_SIZE  = 512\n",
    "BATCH_SIZE  = 4\n",
    "NUM_EPOCHS  = 20\n",
    "LR          = 1e-3\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# Weâ€™ll do **binary** segmentation: induration vs background\n",
    "NUM_CLASSES = 1\n",
    "DEVICE      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c9fb59",
   "metadata": {},
   "source": [
    "## COCO LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57ef1d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images in COCO: 39\n",
      "Total annotations:    78\n",
      "Categories: {0: 'objects', 1: 'induration', 2: 'sticker'}\n"
     ]
    }
   ],
   "source": [
    "with open(COCO_JSON, \"r\") as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "# Build image dict: id -> info\n",
    "images_by_id = {img[\"id\"]: img for img in coco[\"images\"]}\n",
    "\n",
    "# Build annotation lists grouped by image_id\n",
    "anns_by_img = {img_id: [] for img_id in images_by_id.keys()}\n",
    "for ann in coco[\"annotations\"]:\n",
    "    anns_by_img[ann[\"image_id\"]].append(ann)\n",
    "\n",
    "# Category mapping id -> name\n",
    "cat_id_to_name = {c[\"id\"]: c[\"name\"] for c in coco.get(\"categories\", [])}\n",
    "\n",
    "print(f\"Total images in COCO: {len(images_by_id)}\")\n",
    "print(f\"Total annotations:    {len(coco['annotations'])}\")\n",
    "print(\"Categories:\", cat_id_to_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5d250cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_single_mask(ann, img_h, img_w):\n",
    "    \"\"\"\n",
    "    Decode ONE annotation's segmentation into a binary mask (H, W), bool.\n",
    "\n",
    "    Supports:\n",
    "      - RLE dict: {\"size\": [h,w], \"counts\": ...}\n",
    "      - Polygon list: [[x1,y1,...], [...], ...]\n",
    "    \"\"\"\n",
    "    seg = ann[\"segmentation\"]\n",
    "\n",
    "    # --- RLE case ---\n",
    "    if isinstance(seg, dict) and \"counts\" in seg:\n",
    "        rle = seg\n",
    "        # Handle uncompressed RLE (counts as list)\n",
    "        if isinstance(rle[\"counts\"], list):\n",
    "            rle = mask_utils.frPyObjects(rle, rle[\"size\"][0], rle[\"size\"][1])\n",
    "\n",
    "        m = mask_utils.decode(rle)  # (H, W) or (H, W, 1)\n",
    "        if m.ndim == 3:\n",
    "            m = m[:, :, 0]\n",
    "\n",
    "        # If needed, resize to image size (should usually match already)\n",
    "        if m.shape[0] != img_h or m.shape[1] != img_w:\n",
    "            m = cv2.resize(m.astype(np.uint8),\n",
    "                           (img_w, img_h),\n",
    "                           interpolation=cv2.INTER_NEAREST)\n",
    "        return m.astype(bool)\n",
    "\n",
    "    # --- Polygon(s) case ---\n",
    "    elif isinstance(seg, list):\n",
    "        m = np.zeros((img_h, img_w), dtype=np.uint8)\n",
    "        for poly in seg:\n",
    "            pts = np.array(poly, dtype=np.float32).reshape(-1, 2)\n",
    "            pts = pts.reshape((-1, 1, 2)).astype(np.int32)\n",
    "            cv2.fillPoly(m, [pts], 1)\n",
    "        return m.astype(bool)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported segmentation type: {type(seg)}\")\n",
    "\n",
    "\n",
    "def build_induration_mask(img_id, img_h, img_w):\n",
    "    \"\"\"\n",
    "    Build a **binary mask** (H, W) for the 'induration' class only\n",
    "    for a given image_id by unioning all its induration annotations.\n",
    "    \"\"\"\n",
    "    anns = anns_by_img.get(img_id, [])\n",
    "    mask = np.zeros((img_h, img_w), dtype=bool)\n",
    "\n",
    "    for ann in anns:\n",
    "        cat_id = ann[\"category_id\"]\n",
    "        cat_name = cat_id_to_name.get(cat_id, \"\").lower()\n",
    "\n",
    "        # Only keep induration annotations\n",
    "        if \"induration\" not in cat_name:\n",
    "            continue\n",
    "\n",
    "        m = decode_single_mask(ann, img_h, img_w)  # bool\n",
    "        mask |= m\n",
    "\n",
    "    return mask.astype(\"float32\")  # 0.0/1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e10db4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCOSegmentationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that reads images + masks from COCO JSON.\n",
    "\n",
    "    Returns:\n",
    "      image: Tensor (3, H, W), float\n",
    "      mask:  Tensor (1, H, W), float in {0, 1}\n",
    "    \"\"\"\n",
    "    def __init__(self, image_ids, images_by_id, images_dir, transform=None):\n",
    "        self.image_ids = list(image_ids)\n",
    "        self.images_by_id = images_by_id\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        info = self.images_by_id[img_id]\n",
    "        file_name = info[\"file_name\"]\n",
    "        img_path = str(self.images_dir / file_name)\n",
    "\n",
    "        # Load image\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        h, w = image.shape[:2]\n",
    "\n",
    "        # Build binary induration mask from COCO\n",
    "        mask = build_induration_mask(img_id, h, w)  # (H, W), float32 0/1\n",
    "\n",
    "        if self.transform is not None:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented[\"image\"]\n",
    "            mask  = augmented[\"mask\"]\n",
    "\n",
    "        # (H, W) -> (1, H, W)\n",
    "        if mask.ndim == 2:\n",
    "            mask = mask[None, ...]\n",
    "\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28e661a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_transforms(image_size=IMAGE_SIZE):\n",
    "    train_transform = A.Compose(\n",
    "        [\n",
    "            A.LongestMaxSize(max_size=image_size, p=1.0),\n",
    "            A.PadIfNeeded(\n",
    "                min_height=image_size,\n",
    "                min_width=image_size,\n",
    "                border_mode=cv2.BORDER_CONSTANT,\n",
    "                value=0,\n",
    "                mask_value=0,\n",
    "                p=1.0,\n",
    "            ),\n",
    "\n",
    "            A.ShiftScaleRotate(\n",
    "                shift_limit=0.05,\n",
    "                scale_limit=0.1,\n",
    "                rotate_limit=25,\n",
    "                border_mode=cv2.BORDER_REFLECT_101,\n",
    "                value=0,\n",
    "                mask_value=0,\n",
    "                p=0.9,\n",
    "            ),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "\n",
    "            A.OneOf(\n",
    "                [\n",
    "                    A.RandomBrightnessContrast(\n",
    "                        brightness_limit=0.2,\n",
    "                        contrast_limit=0.2,\n",
    "                        p=1.0,\n",
    "                    ),\n",
    "                    A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=1.0),\n",
    "                ],\n",
    "                p=0.7,\n",
    "            ),\n",
    "            A.HueSaturationValue(\n",
    "                hue_shift_limit=5,\n",
    "                sat_shift_limit=10,\n",
    "                val_shift_limit=10,\n",
    "                p=0.3,\n",
    "            ),\n",
    "\n",
    "            A.OneOf(\n",
    "                [\n",
    "                    A.GaussianBlur(blur_limit=(3, 5), p=1.0),\n",
    "                    A.MedianBlur(blur_limit=3, p=1.0),\n",
    "                    A.GaussNoise(var_limit=(10.0, 30.0), p=1.0),\n",
    "                ],\n",
    "                p=0.3,\n",
    "            ),\n",
    "\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                        std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    val_transform = A.Compose(\n",
    "        [\n",
    "            A.LongestMaxSize(max_size=image_size, p=1.0),\n",
    "            A.PadIfNeeded(\n",
    "                min_height=image_size,\n",
    "                min_width=image_size,\n",
    "                border_mode=cv2.BORDER_CONSTANT,\n",
    "                value=0,\n",
    "                mask_value=0,\n",
    "                p=1.0,\n",
    "            ),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                        std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "    return train_transform, val_transform\n",
    "\n",
    "\n",
    "def make_dataloaders(val_split=0.2):\n",
    "    all_img_ids = list(images_by_id.keys())\n",
    "    all_img_ids.sort()  # deterministic order\n",
    "    n_total = len(all_img_ids)\n",
    "    n_val   = max(1, int(n_total * val_split))\n",
    "\n",
    "    train_ids = all_img_ids[:-n_val]\n",
    "    val_ids   = all_img_ids[-n_val:]\n",
    "\n",
    "    train_tf, val_tf = make_transforms()\n",
    "\n",
    "    train_ds = COCOSegmentationDataset(train_ids, images_by_id, IMAGES_DIR, transform=train_tf)\n",
    "    val_ds   = COCOSegmentationDataset(val_ids,   images_by_id, IMAGES_DIR, transform=val_tf)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    print(f\"Train images: {len(train_ds)}, Val images: {len(val_ds)}\")\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f991408f",
   "metadata": {},
   "source": [
    "## U-NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5a06f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(Conv2d => BN => ReLU) * 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscale with MaxPool then DoubleConv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscale then DoubleConv with skip connection\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, 2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # Pad if needed (due to odd size)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                                    diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=1, bilinear=True):\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc   = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64,   128)\n",
    "        self.down2 = Down(128,  256)\n",
    "        self.down3 = Down(256,  512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512,  1024 // factor)\n",
    "        self.up1   = Up(1024,  512 // factor, bilinear)\n",
    "        self.up2   = Up(512,   256 // factor, bilinear)\n",
    "        self.up3   = Up(256,   128 // factor, bilinear)\n",
    "        self.up4   = Up(128,   64, bilinear)\n",
    "        self.outc  = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x  = self.up1(x5, x4)\n",
    "        x  = self.up2(x,  x3)\n",
    "        x  = self.up3(x,  x2)\n",
    "        x  = self.up4(x,  x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fca6a1",
   "metadata": {},
   "source": [
    "## TRAINING AND EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19e2591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(preds, targets, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Dice for binary segmentation.\n",
    "    preds: logits (B,1,H,W)\n",
    "    targets: masks (B,1,H,W), 0/1\n",
    "    \"\"\"\n",
    "    preds = torch.sigmoid(preds)\n",
    "    preds = (preds > 0.5).float()\n",
    "\n",
    "    intersection = (preds * targets).sum(dim=(2, 3))\n",
    "    union = preds.sum(dim=(2, 3)) + targets.sum(dim=(2, 3))\n",
    "\n",
    "    dice = (2.0 * intersection + eps) / (union + eps)\n",
    "    return dice.mean().item()\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, masks in loader:\n",
    "        images = images.to(device)\n",
    "        masks  = masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    return running_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_dice = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in loader:\n",
    "            images = images.to(device)\n",
    "            masks  = masks.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            dice = dice_coefficient(outputs, masks)\n",
    "            running_dice += dice * images.size(0)\n",
    "\n",
    "    val_loss = running_loss / len(loader.dataset)\n",
    "    val_dice = running_dice / len(loader.dataset)\n",
    "    return val_loss, val_dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ec1e50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 32, Val images: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vd/z033x6cd5pv4gp1jt750qvgr0000gn/T/ipykernel_12433/2333751630.py:5: UserWarning: Argument(s) 'value, mask_value' are not valid for transform PadIfNeeded\n",
      "  A.PadIfNeeded(\n",
      "/var/folders/vd/z033x6cd5pv4gp1jt750qvgr0000gn/T/ipykernel_12433/2333751630.py:14: UserWarning: Argument(s) 'value, mask_value' are not valid for transform ShiftScaleRotate\n",
      "  A.ShiftScaleRotate(\n",
      "/var/folders/vd/z033x6cd5pv4gp1jt750qvgr0000gn/T/ipykernel_12433/2333751630.py:47: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
      "  A.GaussNoise(var_limit=(10.0, 30.0), p=1.0),\n",
      "/var/folders/vd/z033x6cd5pv4gp1jt750qvgr0000gn/T/ipykernel_12433/2333751630.py:61: UserWarning: Argument(s) 'value, mask_value' are not valid for transform PadIfNeeded\n",
      "  A.PadIfNeeded(\n",
      "[ WARN:0@309.150] global loadsave.cpp:275 findDecoder imread_('data/images/scene14_img1_JPG.rf.416b282642bc5142cf4a7ce65cf718a2.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Image not found: data/images/scene14_img1_JPG.rf.416b282642bc5142cf4a7ce65cf718a2.jpg",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munet_coco_best.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, NUM_EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 11\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n\u001b[1;32m     12\u001b[0m     val_loss, val_dice \u001b[38;5;241m=\u001b[39m validate(model, val_loader, criterion, DEVICE)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal Dice: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_dice\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 21\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     19\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, masks \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m     22\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     23\u001b[0m     masks  \u001b[38;5;241m=\u001b[39m masks\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    738\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:788\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    787\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 788\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    790\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[20], line 27\u001b[0m, in \u001b[0;36mCOCOSegmentationDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     25\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(img_path, cv2\u001b[38;5;241m.\u001b[39mIMREAD_COLOR)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m     29\u001b[0m h, w \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Image not found: data/images/scene14_img1_JPG.rf.416b282642bc5142cf4a7ce65cf718a2.jpg"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = make_dataloaders(val_split=0.2)\n",
    "\n",
    "model = UNet(n_channels=3, n_classes=NUM_CLASSES).to(DEVICE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "best_val_dice = 0.0\n",
    "save_path = \"unet_coco_best.pth\"\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    val_loss, val_dice = validate(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "    print(f\"Epoch [{epoch}/{NUM_EPOCHS}] \"\n",
    "          f\"Train Loss: {train_loss:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | \"\n",
    "          f\"Val Dice: {val_dice:.4f}\")\n",
    "\n",
    "    if val_dice > best_val_dice:\n",
    "        best_val_dice = val_dice\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"  -> New best model saved to {save_path} (Dice: {best_val_dice:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eef479",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)    \n",
      "exitcode = _main(fd, parent_sentinel)    \n",
      "exitcode = _main(fd, parent_sentinel)\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "                                 ^ ^ ^ ^  ^         ^ ^ ^ ^  ^ ^ ^^ ^^ ^^ ^ ^^^ ^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "\n",
      "^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "^^  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^    ^self = reduction.pickle.load(from_parent)^\n",
      "^^^^^^    ^self = reduction.pickle.load(from_parent)\n",
      "^^^^^^^^^^^^^^^^^^^^ ^ ^ ^ \n",
      "     AttributeError :  Can't get attribute 'COCOSegmentationDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>^^\n",
      "^^^^^^^^^^^^^^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^\n",
      "^^^^^^AttributeError^: ^Can't get attribute 'COCOSegmentationDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>^\n",
      "^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'COCOSegmentationDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'COCOSegmentationDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 12999) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1275\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1275\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_queue\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/multiprocessing/connection.py:440\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 440\u001b[0m     r \u001b[38;5;241m=\u001b[39m wait([\u001b[38;5;28mself\u001b[39m], timeout)\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/multiprocessing/connection.py:1136\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1136\u001b[0m     ready \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39mselect(timeout)\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/signal_handling.py:73\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     _error_if_any_worker_fails()\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 12999) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Take a batch from validation\u001b[39;00m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 9\u001b[0m images, masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(val_loader))\n\u001b[1;32m     10\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     11\u001b[0m masks  \u001b[38;5;241m=\u001b[39m masks\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    738\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1482\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data, worker_id)\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1482\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1485\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1444\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1443\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1444\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_get_data()\n\u001b[1;32m   1445\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1446\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1288\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1287\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1289\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1290\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 12999) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "# Load best model (optional)\n",
    "if os.path.exists(\"unet_coco_best.pth\"):\n",
    "    model.load_state_dict(torch.load(\"unet_coco_best.pth\", map_location=DEVICE))\n",
    "    model.eval()\n",
    "    print(\"Loaded best model from unet_coco_best.pth\")\n",
    "\n",
    "# Take a batch from validation\n",
    "model.eval()\n",
    "images, masks = next(iter(val_loader))\n",
    "images = images.to(DEVICE)\n",
    "masks  = masks.to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(images)\n",
    "    probs  = torch.sigmoid(logits)\n",
    "    preds  = (probs > 0.5).float()\n",
    "\n",
    "images = images.cpu().numpy()\n",
    "masks  = masks.cpu().numpy()\n",
    "preds  = preds.cpu().numpy()\n",
    "\n",
    "# Denormalize images for display\n",
    "mean = np.array([0.485, 0.456, 0.406])[:, None, None]\n",
    "std  = np.array([0.229, 0.224, 0.225])[:, None, None]\n",
    "\n",
    "num_show = min(4, images.shape[0])\n",
    "plt.figure(figsize=(12, 9))\n",
    "\n",
    "for i in range(num_show):\n",
    "    img = images[i]\n",
    "    img = (img * std + mean).transpose(1, 2, 0)\n",
    "    img = np.clip(img, 0, 1)\n",
    "\n",
    "    mask_gt   = masks[i, 0]\n",
    "    mask_pred = preds[i, 0]\n",
    "\n",
    "    plt.subplot(num_show, 3, 3*i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(num_show, 3, 3*i + 2)\n",
    "    plt.imshow(mask_gt, cmap=\"gray\")\n",
    "    plt.title(\"GT Induration\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(num_show, 3, 3*i + 3)\n",
    "    plt.imshow(mask_pred, cmap=\"gray\")\n",
    "    plt.title(\"Pred Induration\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

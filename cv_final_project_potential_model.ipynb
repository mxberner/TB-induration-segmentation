{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaKOm8eMf5aH",
        "outputId": "45ca9bd4-ef74-4d97-8a2a-c1809d3357b8"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m     13\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/My Drive/CV Pictures/RoboFlow COCO JSON\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/__init__.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Tools for model selection, such as cross validation and hyper-parameter tuning.\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_classification_threshold\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     FixedThresholdClassifier,\n\u001b[1;32m      7\u001b[0m     TunedThresholdClassifierCV,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_plot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LearningCurveDisplay, ValidationCurveDisplay\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_search\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV, ParameterGrid, ParameterSampler, RandomizedSearchCV\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_classification_threshold.py:14\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     BaseEstimator,\n\u001b[1;32m      8\u001b[0m     ClassifierMixin,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     clone,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NotFittedError\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     check_scoring,\n\u001b[1;32m     16\u001b[0m     get_scorer_names,\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_scorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _BaseScorer\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _safe_indexing\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Score functions, performance metrics, pairwise metrics and distance computations.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cluster\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_classification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     accuracy_score,\n\u001b[1;32m      6\u001b[0m     balanced_accuracy_score,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     zero_one_loss,\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dist_metrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistanceMetric\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/cluster/__init__.py:25\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bicluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m consensus_score\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_supervised\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     adjusted_mutual_info_score,\n\u001b[1;32m     11\u001b[0m     adjusted_rand_score,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     v_measure_score,\n\u001b[1;32m     24\u001b[0m )\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_unsupervised\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     26\u001b[0m     calinski_harabasz_score,\n\u001b[1;32m     27\u001b[0m     davies_bouldin_score,\n\u001b[1;32m     28\u001b[0m     silhouette_samples,\n\u001b[1;32m     29\u001b[0m     silhouette_score,\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madjusted_mutual_info_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalized_mutual_info_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsensus_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     52\u001b[0m ]\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/cluster/_unsupervised.py:23\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _atol_for_type\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     Interval,\n\u001b[1;32m     20\u001b[0m     StrOptions,\n\u001b[1;32m     21\u001b[0m     validate_params,\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _VALID_METRICS, pairwise_distances, pairwise_distances_chunked\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_number_of_labels\u001b[39m(n_labels, n_samples):\n\u001b[1;32m     27\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that number of labels are valid.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m        Number of samples.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/pairwise.py:50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parallel, delayed\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _num_samples, check_non_negative\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pairwise_distances_reduction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArgKmin\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pairwise_fast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _chi2_kernel_fast, _sparse_manhattan\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Utility Functions\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_pairwise_distances_reduction/__init__.py:94\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Pairwise Distances Reductions\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# =============================\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m#    (see :class:`MiddleTermComputer{32,64}`).\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dispatcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     95\u001b[0m     ArgKmin,\n\u001b[1;32m     96\u001b[0m     ArgKminClassMode,\n\u001b[1;32m     97\u001b[0m     BaseDistancesReductionDispatcher,\n\u001b[1;32m     98\u001b[0m     RadiusNeighbors,\n\u001b[1;32m     99\u001b[0m     RadiusNeighborsClassMode,\n\u001b[1;32m    100\u001b[0m     sqeuclidean_row_norms,\n\u001b[1;32m    101\u001b[0m )\n\u001b[1;32m    103\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseDistancesReductionDispatcher\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgKmin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqeuclidean_row_norms\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    110\u001b[0m ]\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m issparse\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dist_metrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     BOOL_METRICS,\n\u001b[1;32m     10\u001b[0m     METRIC_MAPPING64,\n\u001b[1;32m     11\u001b[0m     DistanceMetric,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_argkmin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     ArgKmin32,\n\u001b[1;32m     15\u001b[0m     ArgKmin64,\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_argkmin_classmode\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     ArgKminClassMode32,\n\u001b[1;32m     19\u001b[0m     ArgKminClassMode64,\n\u001b[1;32m     20\u001b[0m )\n",
            "File \u001b[0;32msklearn/metrics/_dist_metrics.pyx:1\u001b[0m, in \u001b[0;36minit sklearn.metrics._dist_metrics\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import json\n",
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Dataset that reads COCO\n",
        "# =========================\n",
        "\n",
        "class COCOTSTDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for Tuberculin Skin Test (TST) based on COCO annotations.\n",
        "\n",
        "    Assumes:\n",
        "        - category_id == 1: induration\n",
        "        - category_id == 2: sticker (calibration marker)\n",
        "        - physical sticker diameter = 6 mm\n",
        "\n",
        "    For each image:\n",
        "        1. Find sticker bbox -> estimate sticker diameter in pixels.\n",
        "        2. Compute mm_per_px = 6.0 / sticker_px.\n",
        "        3. Find induration area/bbox -> estimate diameter in pixels.\n",
        "        4. Convert to mm and assign label:\n",
        "            label = 1 if diameter_mm >= threshold_mm else 0\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        coco_json_path,\n",
        "        image_root,\n",
        "        ind_cat_id=1,\n",
        "        sticker_cat_id=2,\n",
        "        threshold_mm=5.0,\n",
        "        use_area=True,\n",
        "        transform=None,\n",
        "    ):\n",
        "        with open(coco_json_path, \"r\") as f:\n",
        "            coco = json.load(f)\n",
        "\n",
        "        self.image_root = image_root\n",
        "        self.threshold_mm = threshold_mm\n",
        "\n",
        "        images = {img[\"id\"]: img for img in coco[\"images\"]}\n",
        "        annos_by_img = defaultdict(list)\n",
        "        for a in coco[\"annotations\"]:\n",
        "            annos_by_img[a[\"image_id\"]].append(a)\n",
        "\n",
        "        self.samples = []\n",
        "        for img_id, img in images.items():\n",
        "            annos = annos_by_img.get(img_id, [])\n",
        "\n",
        "            sticker = None\n",
        "            induration = None\n",
        "            for a in annos:\n",
        "                if a[\"category_id\"] == sticker_cat_id:\n",
        "                    sticker = a\n",
        "                elif a[\"category_id\"] == ind_cat_id:\n",
        "                    induration = a\n",
        "\n",
        "            # Skip if missing either annotation\n",
        "            if sticker is None or induration is None:\n",
        "                continue\n",
        "\n",
        "            # --- Sticker diameter in pixels (average of bbox w/h) ---\n",
        "            sx, sy, sw, sh = sticker[\"bbox\"]\n",
        "            sticker_px = (sw + sh) / 2.0\n",
        "            if sticker_px <= 0:\n",
        "                continue\n",
        "\n",
        "            mm_per_px = 6.0 / sticker_px  # 6 mm calibration marker\n",
        "\n",
        "            # --- Induration diameter in pixels ---\n",
        "            if use_area and induration.get(\"area\", 0) > 0:\n",
        "                A = float(induration[\"area\"])\n",
        "                ind_px = 2.0 * math.sqrt(A / math.pi)  # equivalent circle\n",
        "            else:\n",
        "                bx, by, bw, bh = induration[\"bbox\"]\n",
        "                ind_px = max(bw, bh)\n",
        "\n",
        "            diameter_mm = ind_px * mm_per_px\n",
        "            label = 1.0 if diameter_mm >= threshold_mm else 0.0\n",
        "\n",
        "            self.samples.append(\n",
        "                {\n",
        "                    \"image_path\": os.path.join(image_root, img[\"file_name\"]),\n",
        "                    \"diameter_mm\": diameter_mm,\n",
        "                    \"label\": label,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        if transform is None:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((256, 256)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.samples[idx]\n",
        "        img = Image.open(s[\"image_path\"]).convert(\"RGB\")\n",
        "        x = self.transform(img)\n",
        "        y = torch.tensor(s[\"label\"], dtype=torch.float32)\n",
        "        return x, y\n",
        "\n",
        "\n",
        "# =========================\n",
        "# U-Net backbone (encoder) + classifier head\n",
        "# =========================\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(Conv2d -> BN -> ReLU) * 2\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class UNetClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    U-Net encoder used as a feature extractor + global pooling + FC for\n",
        "    binary classification (TB present if induration >= 5mm).\n",
        "    \"\"\"\n",
        "    def __init__(self, n_channels=3):\n",
        "        super().__init__()\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        self.down4 = Down(512, 1024)\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(1024, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)      # 64\n",
        "        x2 = self.down1(x1)   # 128\n",
        "        x3 = self.down2(x2)   # 256\n",
        "        x4 = self.down3(x3)   # 512\n",
        "        x5 = self.down4(x4)   # 1024 (bottleneck)\n",
        "\n",
        "        pooled = self.global_pool(x5)      # [B, 1024, 1, 1]\n",
        "        pooled = pooled.view(pooled.size(0), -1)  # [B, 1024]\n",
        "        logits = self.fc(pooled)           # [B, 1]\n",
        "        return logits.squeeze(1)           # [B]\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Training / evaluation\n",
        "# =========================\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for imgs, labels in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(imgs)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "        probs = torch.sigmoid(logits)\n",
        "        preds = (probs >= 0.5).float()\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "\n",
        "def eval_one_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in loader:\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            running_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "            probs = torch.sigmoid(logits)\n",
        "            preds = (probs >= 0.5).float()\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Main\n",
        "# =========================\n",
        "\n",
        "def main():\n",
        "\n",
        "    COCO_JSON = \"data372/_annotations.coco.json\"  # path to your COCO file\n",
        "    IMAGE_ROOT = \"data372\"                 # folder containing the image files\n",
        "\n",
        "    batch_size = 8\n",
        "    num_epochs = 20\n",
        "    lr = 1e-3\n",
        "    val_split = 0.2     # 80/20 split\n",
        "    threshold_mm = 5.0  # TB positive if diameter >= 5mm\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    dataset = COCOTSTDataset(\n",
        "        coco_json_path=COCO_JSON,\n",
        "        image_root=IMAGE_ROOT,\n",
        "        threshold_mm=threshold_mm,\n",
        "        use_area=True,\n",
        "    )\n",
        "\n",
        "    print(f\"Total usable images: {len(dataset)}\")\n",
        "\n",
        "    # Extract labels for stratified split\n",
        "    labels = [s[\"label\"] for s in dataset.samples]\n",
        "    indices = list(range(len(dataset)))\n",
        "\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        indices,\n",
        "        test_size=val_split,\n",
        "        random_state=42,\n",
        "        stratify=labels\n",
        "    )\n",
        "\n",
        "    train_dataset = Subset(dataset, train_idx)\n",
        "    val_dataset = Subset(dataset, val_idx)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                              shuffle=True, num_workers=4, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
        "                            shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    model = UNetClassifier(n_channels=3).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_loss, train_acc = train_one_epoch(\n",
        "            model, train_loader, optimizer, criterion, device\n",
        "        )\n",
        "        val_loss, val_acc = eval_one_epoch(\n",
        "            model, val_loader, criterion, device\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:02d}/{num_epochs} | \"\n",
        "            f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | \"\n",
        "            f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "\n",
        "    # =========================\n",
        "    # Final Accuracy (CIFAR-style)\n",
        "    # =========================\n",
        "\n",
        "    print(\"\\nEvaluating final model accuracy...\")\n",
        "\n",
        "    test_loader = DataLoader(dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for inputs, labels in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        logits = model(inputs)\n",
        "\n",
        "        # Convert logits → probabilities → predicted class (0 or 1)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        predicted = (probs >= 0.5).float()\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"\\nFinal Model Accuracy: {correct / total:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
